{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de regressão com árvores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#training procedures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giova\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv('data/ENEM_CLEAN.csv')\n",
    "df = pd.read_csv('data/ENEM_CLEAN_WITH_NAN.csv')\n",
    "df = df.loc[:, ~(df == 'FALTANTE').any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos com árvore de decisão\n",
    "\n",
    "Iremos avaliar modelos utilizando três conjuntos diferentes dos dados, as duas primeiras serão utilizando a biblioteca Scikit-learn e a segunda utilizando H20.\n",
    "\n",
    "- Apenas as variáveis numérica.\n",
    "- Variáveis numérica e categórica com one hot encoding (Scikit-lean).\n",
    "- Variáveis numérica e categórica com one hot encoding (H20)\n",
    "\n",
    "A implementaçã do Scikit-learn de árvores de decisão considera apenas variáveis numéricas, o algoritmo não é capaz de considerar divisões adequadas na hora de decidir se irá criar mais _branchs_, dessa forma, ao utilizar _one hot encoding_, se trata de uma aproximação da solução do problema com variáveis categóricas.\n",
    "\n",
    "### Variáveis numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[col for col in df.columns if col[0:3] == 'NUM']].drop(columns = ['NUM_NOTA'])\n",
    "X_columns_names = X.columns\n",
    "X = X.values\n",
    "Y = df.NUM_NOTA.values\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.,  0.,  5., ...,  1.,  2.,  1.],\n",
       "       [17.,  0.,  3., ...,  0.,  3.,  0.],\n",
       "       [22.,  0.,  3., ...,  1.,  2.,  0.],\n",
       "       ...,\n",
       "       [18.,  1.,  3., ...,  1.,  1.,  0.],\n",
       "       [17.,  0.,  3., ...,  1.,  1.,  0.],\n",
       "       [17.,  0.,  3., ...,  2.,  3.,  1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On iteration 10.\n",
      "Best train score: 0.249\n",
      "On iteration 20.\n",
      "Best train score: 0.249\n",
      "On iteration 30.\n",
      "Best train score: 0.249\n",
      "On iteration 40.\n",
      "Best train score: 0.249\n",
      "On iteration 50.\n",
      "Best train score: 0.249\n",
      "On iteration 60.\n",
      "Best train score: 0.249\n",
      "On iteration 70.\n",
      "Best train score: 0.249\n",
      "On iteration 80.\n",
      "Best train score: 0.249\n",
      "On iteration 90.\n",
      "Best train score: 0.249\n",
      "On iteration 100.\n",
      "Best train score: 0.252\n",
      "On iteration 110.\n",
      "Best train score: 0.252\n",
      "On iteration 120.\n",
      "Best train score: 0.252\n",
      "On iteration 130.\n",
      "Best train score: 0.252\n",
      "On iteration 140.\n",
      "Best train score: 0.252\n",
      "On iteration 150.\n",
      "Best train score: 0.252\n",
      "On iteration 160.\n",
      "Best train score: 0.252\n",
      "On iteration 170.\n",
      "Best train score: 0.252\n",
      "On iteration 180.\n",
      "Best train score: 0.252\n",
      "On iteration 190.\n",
      "Best train score: 0.252\n",
      "On iteration 200.\n",
      "Best train score: 0.252\n",
      "On iteration 210.\n",
      "Best train score: 0.252\n",
      "On iteration 220.\n",
      "Best train score: 0.252\n",
      "On iteration 230.\n",
      "Best train score: 0.252\n",
      "On iteration 240.\n",
      "Best train score: 0.252\n",
      "On iteration 250.\n",
      "Best train score: 0.252\n",
      "On iteration 260.\n",
      "Best train score: 0.252\n",
      "On iteration 270.\n",
      "Best train score: 0.252\n",
      "On iteration 280.\n",
      "Best train score: 0.252\n",
      "On iteration 290.\n",
      "Best train score: 0.252\n",
      "On iteration 300.\n",
      "Best train score: 0.252\n",
      "On iteration 310.\n",
      "Best train score: 0.252\n",
      "On iteration 320.\n",
      "Best train score: 0.252\n",
      "On iteration 330.\n",
      "Best train score: 0.252\n",
      "On iteration 340.\n",
      "Best train score: 0.252\n",
      "On iteration 350.\n",
      "Best train score: 0.252\n",
      "On iteration 360.\n",
      "Best train score: 0.252\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "i = 0\n",
    "for max_depth in np.arange(10, 20, 3):\n",
    "    for min_samples_split in np.arange(20, 45, 3):\n",
    "        for min_samples_leaf in np.arange(30, 60, 3):\n",
    "            model = tree.DecisionTreeRegressor(max_depth= max_depth,\n",
    "                                              min_samples_split = min_samples_split,\n",
    "                                              min_samples_leaf = min_samples_leaf)\n",
    "            model.fit(x_train, y_train)\n",
    "            results.append([max_depth, \n",
    "                            min_samples_split, \n",
    "                            min_samples_leaf, \n",
    "                            model.score(x_train, y_train),\n",
    "                            model.score(x_test, y_test)])\n",
    "            i+= 1\n",
    "            if i % 10 == 0:\n",
    "                print(f\"On iteration {i}.\")\n",
    "                print(f\"Best train score: {max([ii[4] for ii in results]):.3f}\")\n",
    "                \n",
    "decision_tree_numeric_vars_results = pd.DataFrame(results, columns = ['max_depth',\n",
    "                                                            'min_samples_split',\n",
    "                                                            'min_samples_leaf',\n",
    "                                                            'r2_train',\n",
    "                                                            'r2_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaft</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>54</td>\n",
       "      <td>0.273526</td>\n",
       "      <td>0.252010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "      <td>0.273526</td>\n",
       "      <td>0.252010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>13</td>\n",
       "      <td>29</td>\n",
       "      <td>54</td>\n",
       "      <td>0.273526</td>\n",
       "      <td>0.252010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>13</td>\n",
       "      <td>41</td>\n",
       "      <td>54</td>\n",
       "      <td>0.273526</td>\n",
       "      <td>0.252010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>13</td>\n",
       "      <td>38</td>\n",
       "      <td>54</td>\n",
       "      <td>0.273526</td>\n",
       "      <td>0.252010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>0.297657</td>\n",
       "      <td>0.241018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>19</td>\n",
       "      <td>44</td>\n",
       "      <td>30</td>\n",
       "      <td>0.297657</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>19</td>\n",
       "      <td>38</td>\n",
       "      <td>30</td>\n",
       "      <td>0.297657</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>0.297657</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>0.297657</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     max_depth  min_samples_split  min_samples_leaft  r2_train   r2_test\n",
       "138         13                 32                 54  0.273526  0.252010\n",
       "178         13                 44                 54  0.273526  0.252010\n",
       "128         13                 29                 54  0.273526  0.252010\n",
       "168         13                 41                 54  0.273526  0.252010\n",
       "158         13                 38                 54  0.273526  0.252010\n",
       "..         ...                ...                ...       ...       ...\n",
       "270         19                 20                 30  0.297657  0.241018\n",
       "350         19                 44                 30  0.297657  0.241000\n",
       "330         19                 38                 30  0.297657  0.241000\n",
       "310         19                 32                 30  0.297657  0.241000\n",
       "290         19                 26                 30  0.297657  0.241000\n",
       "\n",
       "[360 rows x 5 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree_numeric_vars_results.sort_values('r2_test', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_BANHEIRO: 0.390\n",
      "NUM_COMPUTADOR: 0.268\n",
      "NUM_IDADE: 0.089\n",
      "NUM_ANO_CONCLUIU: 0.073\n",
      "NUM_EMPREGADO_DOMESTICO: 0.060\n",
      "NUM_PESSOAS_RESIDENCIA: 0.042\n",
      "NUM_FREEZER: 0.020\n",
      "NUM_CARRO: 0.020\n",
      "NUM_CELULAR: 0.013\n",
      "NUM_MOTO: 0.005\n",
      "NUM_LAVAR_LOUCA: 0.005\n",
      "NUM_TV: 0.004\n",
      "NUM_QUARTOS: 0.003\n",
      "NUM_MAQUINA_SECAR: 0.003\n",
      "NUM_GELADEIRA: 0.001\n",
      "NUM_MICRO_ONDAS: 0.001\n",
      "NUM_MAQUINA_LAVAR: 0.001\n"
     ]
    }
   ],
   "source": [
    "model = tree.DecisionTreeRegressor(max_depth = 13, min_samples_split = 32, min_samples_leaf = 54)\n",
    "model.fit(x_train, y_train)\n",
    "columns_importances = list(zip(X_columns_names, model.feature_importances_))\n",
    "columns_importances.sort(key = lambda x : x[1], reverse = True)\n",
    "for i in range(len(X_columns_names)):\n",
    "    print(f\"{columns_importances[i][0]}: {columns_importances[i][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Adaboost\n",
    "\n",
    "### Variáveis numérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=50 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=50, score=0.186, total=  37.1s\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=50 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   37.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=50, score=0.183, total=  36.0s\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=50 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=50, score=0.186, total=  37.3s\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=50, score=0.184, total=  36.4s\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=50, score=0.185, total=  38.3s\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=100, score=0.186, total= 1.4min\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=100, score=0.184, total= 1.6min\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=100, score=0.186, total= 1.5min\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=100, score=0.185, total= 1.5min\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=100, score=0.186, total= 1.6min\n",
      "[CV] learning_rate=0.01, loss=square, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.01, loss=square, n_estimators=50, score=0.182, total=  45.1s\n",
      "[CV] learning_rate=0.01, loss=square, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.01, loss=square, n_estimators=50, score=0.180, total=  49.7s\n",
      "[CV] learning_rate=0.01, loss=square, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.01, loss=square, n_estimators=50, score=0.182, total=  42.8s\n",
      "[CV] learning_rate=0.01, loss=square, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.01, loss=square, n_estimators=50, score=0.184, total=  49.8s\n",
      "[CV] learning_rate=0.01, loss=square, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.01, loss=square, n_estimators=50, score=0.185, total=  43.8s\n",
      "[CV] learning_rate=0.01, loss=square, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.01, loss=square, n_estimators=100, score=0.186, total= 1.4min\n",
      "[CV] learning_rate=0.01, loss=square, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.01, loss=square, n_estimators=100, score=0.183, total= 1.6min\n",
      "[CV] learning_rate=0.01, loss=square, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.01, loss=square, n_estimators=100, score=0.186, total= 1.4min\n",
      "[CV] learning_rate=0.01, loss=square, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.01, loss=square, n_estimators=100, score=0.184, total= 1.4min\n",
      "[CV] learning_rate=0.01, loss=square, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.01, loss=square, n_estimators=100, score=0.186, total= 1.4min\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=50 ...........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=50, score=0.182, total=  48.7s\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=50 ...........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=50, score=0.183, total=  45.8s\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=50 ...........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=50, score=0.186, total=  46.1s\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=50 ...........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=50, score=0.184, total=  46.7s\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=50 ...........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=50, score=0.185, total=  46.9s\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=100, score=0.186, total= 1.5min\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=100, score=0.183, total= 1.5min\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=100, score=0.186, total= 1.5min\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=100, score=0.184, total= 1.6min\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=100, score=0.186, total= 1.7min\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=50, score=0.195, total=  46.8s\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=50, score=0.189, total=  42.4s\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=50, score=0.194, total=  47.5s\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=50, score=0.195, total=  47.1s\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=50, score=0.194, total=  42.6s\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=100, score=0.199, total= 1.4min\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=100, score=0.197, total= 1.4min\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=100, score=0.200, total= 1.5min\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=100, score=0.200, total= 1.5min\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=100, score=0.198, total= 1.6min\n",
      "[CV] learning_rate=0.05, loss=square, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.05, loss=square, n_estimators=50, score=0.181, total=  45.4s\n",
      "[CV] learning_rate=0.05, loss=square, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.05, loss=square, n_estimators=50, score=0.179, total=  47.7s\n",
      "[CV] learning_rate=0.05, loss=square, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.05, loss=square, n_estimators=50, score=0.181, total=  47.3s\n",
      "[CV] learning_rate=0.05, loss=square, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.05, loss=square, n_estimators=50, score=0.187, total=  58.4s\n",
      "[CV] learning_rate=0.05, loss=square, n_estimators=50 ................\n",
      "[CV]  learning_rate=0.05, loss=square, n_estimators=50, score=0.183, total=  45.8s\n",
      "[CV] learning_rate=0.05, loss=square, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=square, n_estimators=100, score=0.185, total= 1.9min\n",
      "[CV] learning_rate=0.05, loss=square, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=square, n_estimators=100, score=0.183, total= 1.6min\n",
      "[CV] learning_rate=0.05, loss=square, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=square, n_estimators=100, score=0.185, total= 1.6min\n",
      "[CV] learning_rate=0.05, loss=square, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=square, n_estimators=100, score=0.184, total= 1.7min\n",
      "[CV] learning_rate=0.05, loss=square, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=square, n_estimators=100, score=0.184, total= 1.8min\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=50 ...........\n",
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=50, score=0.193, total=  49.0s\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=50 ...........\n",
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=50, score=0.192, total=  49.6s\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=50 ...........\n",
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=50, score=0.194, total=  52.4s\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=50 ...........\n",
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=50, score=0.193, total=  46.7s\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=50 ...........\n",
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=50, score=0.189, total=  48.0s\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=100 ..........\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=100, score=0.200, total= 1.5min\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=100, score=0.200, total= 1.6min\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=100, score=0.202, total= 1.5min\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=100, score=0.199, total= 1.6min\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=100, score=0.200, total= 1.6min\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=50, score=0.200, total=  41.8s\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=50, score=0.197, total=  46.7s\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=50, score=0.201, total=  43.5s\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=50, score=0.199, total=  43.8s\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=50, score=0.199, total=  41.4s\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=100, score=0.195, total= 1.5min\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=100, score=0.191, total= 1.4min\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=100, score=0.194, total= 1.4min\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=100, score=0.198, total= 1.4min\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=100, score=0.195, total= 1.4min\n",
      "[CV] learning_rate=0.1, loss=square, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.1, loss=square, n_estimators=50, score=0.185, total=  42.2s\n",
      "[CV] learning_rate=0.1, loss=square, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.1, loss=square, n_estimators=50, score=0.185, total=  42.6s\n",
      "[CV] learning_rate=0.1, loss=square, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.1, loss=square, n_estimators=50, score=0.187, total=  43.0s\n",
      "[CV] learning_rate=0.1, loss=square, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.1, loss=square, n_estimators=50, score=0.186, total=  44.4s\n",
      "[CV] learning_rate=0.1, loss=square, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.1, loss=square, n_estimators=50, score=0.185, total=  46.3s\n",
      "[CV] learning_rate=0.1, loss=square, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=square, n_estimators=100, score=0.167, total= 1.6min\n",
      "[CV] learning_rate=0.1, loss=square, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=square, n_estimators=100, score=0.173, total= 1.6min\n",
      "[CV] learning_rate=0.1, loss=square, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=square, n_estimators=100, score=0.171, total= 1.4min\n",
      "[CV] learning_rate=0.1, loss=square, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=square, n_estimators=100, score=0.152, total= 1.5min\n",
      "[CV] learning_rate=0.1, loss=square, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=square, n_estimators=100, score=0.168, total= 1.5min\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=50 ............\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=50, score=0.202, total=  43.8s\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=50 ............\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=50, score=0.200, total=  37.5s\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=50 ............\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=50, score=0.202, total=  34.5s\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=50 ............\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=50, score=0.201, total=  35.1s\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=50 ............\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=50, score=0.200, total=  32.2s\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=100, score=0.203, total= 1.0min\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=100, score=0.199, total= 1.0min\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=100, score=0.201, total= 1.0min\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=100, score=0.204, total= 1.0min\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=100, score=0.202, total= 1.7min\n",
      "[CV] learning_rate=0.3, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.3, loss=linear, n_estimators=50, score=0.187, total=  36.9s\n",
      "[CV] learning_rate=0.3, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.3, loss=linear, n_estimators=50, score=0.184, total=  36.5s\n",
      "[CV] learning_rate=0.3, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.3, loss=linear, n_estimators=50, score=0.182, total=  37.0s\n",
      "[CV] learning_rate=0.3, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.3, loss=linear, n_estimators=50, score=0.187, total=  35.5s\n",
      "[CV] learning_rate=0.3, loss=linear, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.3, loss=linear, n_estimators=50, score=0.185, total=  36.1s\n",
      "[CV] learning_rate=0.3, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.3, loss=linear, n_estimators=100, score=0.168, total= 1.1min\n",
      "[CV] learning_rate=0.3, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.3, loss=linear, n_estimators=100, score=0.164, total= 1.2min\n",
      "[CV] learning_rate=0.3, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.3, loss=linear, n_estimators=100, score=0.167, total= 1.2min\n",
      "[CV] learning_rate=0.3, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.3, loss=linear, n_estimators=100, score=0.173, total= 1.2min\n",
      "[CV] learning_rate=0.3, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.3, loss=linear, n_estimators=100, score=0.168, total= 1.2min\n",
      "[CV] learning_rate=0.3, loss=square, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.3, loss=square, n_estimators=50, score=0.167, total=  34.8s\n",
      "[CV] learning_rate=0.3, loss=square, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.3, loss=square, n_estimators=50, score=0.167, total=  34.4s\n",
      "[CV] learning_rate=0.3, loss=square, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.3, loss=square, n_estimators=50, score=0.155, total=  35.0s\n",
      "[CV] learning_rate=0.3, loss=square, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.3, loss=square, n_estimators=50, score=0.138, total=  35.4s\n",
      "[CV] learning_rate=0.3, loss=square, n_estimators=50 .................\n",
      "[CV]  learning_rate=0.3, loss=square, n_estimators=50, score=0.157, total=  31.2s\n",
      "[CV] learning_rate=0.3, loss=square, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.3, loss=square, n_estimators=100, score=0.145, total=  57.8s\n",
      "[CV] learning_rate=0.3, loss=square, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.3, loss=square, n_estimators=100, score=0.161, total=  58.0s\n",
      "[CV] learning_rate=0.3, loss=square, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.3, loss=square, n_estimators=100, score=0.152, total=  58.0s\n",
      "[CV] learning_rate=0.3, loss=square, n_estimators=100 ................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.3, loss=square, n_estimators=100, score=0.089, total=  58.2s\n",
      "[CV] learning_rate=0.3, loss=square, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.3, loss=square, n_estimators=100, score=0.136, total=  58.5s\n",
      "[CV] learning_rate=0.3, loss=exponential, n_estimators=50 ............\n",
      "[CV]  learning_rate=0.3, loss=exponential, n_estimators=50, score=0.200, total=  30.8s\n",
      "[CV] learning_rate=0.3, loss=exponential, n_estimators=50 ............\n",
      "[CV]  learning_rate=0.3, loss=exponential, n_estimators=50, score=0.196, total=  30.8s\n",
      "[CV] learning_rate=0.3, loss=exponential, n_estimators=50 ............\n",
      "[CV]  learning_rate=0.3, loss=exponential, n_estimators=50, score=0.196, total=  30.8s\n",
      "[CV] learning_rate=0.3, loss=exponential, n_estimators=50 ............\n",
      "[CV]  learning_rate=0.3, loss=exponential, n_estimators=50, score=0.201, total=  30.7s\n",
      "[CV] learning_rate=0.3, loss=exponential, n_estimators=50 ............\n",
      "[CV]  learning_rate=0.3, loss=exponential, n_estimators=50, score=0.197, total=  30.8s\n",
      "[CV] learning_rate=0.3, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.3, loss=exponential, n_estimators=100, score=0.181, total= 1.0min\n",
      "[CV] learning_rate=0.3, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.3, loss=exponential, n_estimators=100, score=0.175, total= 1.0min\n",
      "[CV] learning_rate=0.3, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.3, loss=exponential, n_estimators=100, score=0.175, total= 1.0min\n",
      "[CV] learning_rate=0.3, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.3, loss=exponential, n_estimators=100, score=0.185, total= 1.0min\n",
      "[CV] learning_rate=0.3, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.3, loss=exponential, n_estimators=100, score=0.178, total= 1.0min\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=50 ...................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=50, score=0.148, total=  29.4s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=50 ...................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=50, score=0.149, total=  29.7s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=50 ...................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=50, score=0.171, total=  13.2s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=50 ...................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=50, score=0.171, total=  22.2s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=50 ...................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=50, score=0.143, total=  29.8s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=100 ..................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=100, score=0.130, total=  46.1s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=100 ..................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=100, score=0.103, total=  47.9s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=100 ..................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=100, score=0.114, total=  57.3s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=100 ..................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=100, score=0.165, total=  25.0s\n",
      "[CV] learning_rate=1, loss=linear, n_estimators=100 ..................\n",
      "[CV]  learning_rate=1, loss=linear, n_estimators=100, score=0.119, total=  38.8s\n",
      "[CV] learning_rate=1, loss=square, n_estimators=50 ...................\n",
      "[CV]  learning_rate=1, loss=square, n_estimators=50, score=0.117, total=  28.0s\n",
      "[CV] learning_rate=1, loss=square, n_estimators=50 ...................\n",
      "[CV]  learning_rate=1, loss=square, n_estimators=50, score=0.151, total=  28.1s\n",
      "[CV] learning_rate=1, loss=square, n_estimators=50 ...................\n",
      "[CV]  learning_rate=1, loss=square, n_estimators=50, score=0.115, total=  28.2s\n",
      "[CV] learning_rate=1, loss=square, n_estimators=50 ...................\n",
      "[CV]  learning_rate=1, loss=square, n_estimators=50, score=0.049, total=  28.3s\n",
      "[CV] learning_rate=1, loss=square, n_estimators=50 ...................\n",
      "[CV]  learning_rate=1, loss=square, n_estimators=50, score=0.130, total=  28.1s\n",
      "[CV] learning_rate=1, loss=square, n_estimators=100 ..................\n",
      "[CV]  learning_rate=1, loss=square, n_estimators=100, score=0.046, total=  53.9s\n",
      "[CV] learning_rate=1, loss=square, n_estimators=100 ..................\n",
      "[CV]  learning_rate=1, loss=square, n_estimators=100, score=0.113, total=  54.2s\n",
      "[CV] learning_rate=1, loss=square, n_estimators=100 ..................\n",
      "[CV]  learning_rate=1, loss=square, n_estimators=100, score=0.051, total=  54.2s\n",
      "[CV] learning_rate=1, loss=square, n_estimators=100 ..................\n",
      "[CV]  learning_rate=1, loss=square, n_estimators=100, score=0.046, total=  54.8s\n",
      "[CV] learning_rate=1, loss=square, n_estimators=100 ..................\n",
      "[CV]  learning_rate=1, loss=square, n_estimators=100, score=0.037, total=  55.2s\n",
      "[CV] learning_rate=1, loss=exponential, n_estimators=50 ..............\n",
      "[CV]  learning_rate=1, loss=exponential, n_estimators=50, score=0.157, total=  29.9s\n",
      "[CV] learning_rate=1, loss=exponential, n_estimators=50 ..............\n",
      "[CV]  learning_rate=1, loss=exponential, n_estimators=50, score=0.151, total=  29.9s\n",
      "[CV] learning_rate=1, loss=exponential, n_estimators=50 ..............\n",
      "[CV]  learning_rate=1, loss=exponential, n_estimators=50, score=0.156, total=  29.8s\n",
      "[CV] learning_rate=1, loss=exponential, n_estimators=50 ..............\n",
      "[CV]  learning_rate=1, loss=exponential, n_estimators=50, score=0.169, total=  29.9s\n",
      "[CV] learning_rate=1, loss=exponential, n_estimators=50 ..............\n",
      "[CV]  learning_rate=1, loss=exponential, n_estimators=50, score=0.146, total=  30.0s\n",
      "[CV] learning_rate=1, loss=exponential, n_estimators=100 .............\n",
      "[CV]  learning_rate=1, loss=exponential, n_estimators=100, score=0.108, total=  57.9s\n",
      "[CV] learning_rate=1, loss=exponential, n_estimators=100 .............\n",
      "[CV]  learning_rate=1, loss=exponential, n_estimators=100, score=0.084, total=  58.1s\n",
      "[CV] learning_rate=1, loss=exponential, n_estimators=100 .............\n",
      "[CV]  learning_rate=1, loss=exponential, n_estimators=100, score=0.090, total=  57.9s\n",
      "[CV] learning_rate=1, loss=exponential, n_estimators=100 .............\n",
      "[CV]  learning_rate=1, loss=exponential, n_estimators=100, score=0.124, total=  59.4s\n",
      "[CV] learning_rate=1, loss=exponential, n_estimators=100 .............\n",
      "[CV]  learning_rate=1, loss=exponential, n_estimators=100, score=0.052, total=  58.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed: 144.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=AdaBoostRegressor(),\n",
       "             param_grid={'learning_rate': [0.01, 0.05, 0.1, 0.3, 1],\n",
       "                         'loss': ['linear', 'square', 'exponential'],\n",
       "                         'n_estimators': [50, 100]},\n",
       "             scoring='r2', verbose=2.5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training with numeric variables\n",
    "X = df[[col for col in df.columns if col[0:3] == 'NUM']].drop(columns = ['NUM_NOTA'])\n",
    "X_columns_names = X.columns\n",
    "X = X.values\n",
    "Y = df.NUM_NOTA.values\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 1)\n",
    "\n",
    "params = {\n",
    " 'n_estimators': [50, 100],\n",
    " 'learning_rate' : [0.01,0.05,0.1,0.3,1],\n",
    " 'loss' : ['linear', 'square', 'exponential']\n",
    "}\n",
    "\n",
    "adaboost_tunning = GridSearchCV(AdaBoostRegressor(), \n",
    "             param_grid = params,\n",
    "             scoring= 'r2',\n",
    "             verbose= 2.5)\n",
    "\n",
    "adaboost_tunning.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_tunning_df = pd.DataFrame(adaboost_tunning.cv_results_).sort_values('rank_test_score')\n",
    "adaboost_tunning_df.to_csv('results/adaboost_tunning_numeric.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variáveis numérica e categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=100, score=0.211, total= 5.3min\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=100, score=0.210, total= 5.2min\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=100 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 10.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=100, score=0.211, total= 5.2min\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=100, score=0.214, total= 5.2min\n",
      "[CV] learning_rate=0.01, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.01, loss=linear, n_estimators=100, score=0.213, total= 5.2min\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=100, score=0.211, total= 5.3min\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=100, score=0.209, total= 5.2min\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=100, score=0.211, total= 5.0min\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=100, score=0.214, total= 5.0min\n",
      "[CV] learning_rate=0.01, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.01, loss=exponential, n_estimators=100, score=0.213, total= 5.2min\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=100, score=0.255, total= 5.3min\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=100, score=0.249, total= 5.3min\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=100, score=0.251, total= 5.1min\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=100, score=0.259, total= 5.1min\n",
      "[CV] learning_rate=0.05, loss=linear, n_estimators=100 ...............\n",
      "[CV]  learning_rate=0.05, loss=linear, n_estimators=100, score=0.253, total= 5.1min\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=100, score=0.249, total= 5.2min\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=100, score=0.248, total= 5.3min\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=100, score=0.247, total= 5.2min\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=100, score=0.255, total= 5.3min\n",
      "[CV] learning_rate=0.05, loss=exponential, n_estimators=100 ..........\n",
      "[CV]  learning_rate=0.05, loss=exponential, n_estimators=100, score=0.251, total= 5.2min\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=100, score=0.263, total= 5.0min\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=100, score=0.260, total= 5.0min\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=100, score=0.256, total= 5.1min\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=100, score=0.269, total= 5.1min\n",
      "[CV] learning_rate=0.1, loss=linear, n_estimators=100 ................\n",
      "[CV]  learning_rate=0.1, loss=linear, n_estimators=100, score=0.259, total= 5.1min\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=100, score=0.272, total= 5.2min\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=100, score=0.270, total= 5.2min\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=100, score=0.269, total= 5.2min\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=100, score=0.274, total= 5.2min\n",
      "[CV] learning_rate=0.1, loss=exponential, n_estimators=100 ...........\n",
      "[CV]  learning_rate=0.1, loss=exponential, n_estimators=100, score=0.268, total= 5.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 155.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=AdaBoostRegressor(),\n",
       "             param_grid={'learning_rate': [0.01, 0.05, 0.1],\n",
       "                         'loss': ['linear', 'exponential'],\n",
       "                         'n_estimators': [100]},\n",
       "             scoring='r2', verbose=2.5)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training with numeric and categoric variables\n",
    "X = pd.get_dummies(df, drop_first = True).drop(columns = ['NUM_NOTA'])\n",
    "X_columns_names = X.columns\n",
    "X = X.values\n",
    "Y = df.NUM_NOTA.values\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 1)\n",
    "\n",
    "params = {\n",
    " 'n_estimators': [100],\n",
    " 'learning_rate' : [0.01,0.05,0.1],\n",
    " 'loss' : ['linear', 'exponential']\n",
    "}\n",
    "\n",
    "adaboost_tunning = GridSearchCV(AdaBoostRegressor(), \n",
    "             param_grid = params,\n",
    "             scoring= 'r2',\n",
    "             verbose= 2.5)\n",
    "\n",
    "adaboost_tunning.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth:2\n",
      "Train score: 0.166\n",
      "Test score: 0.162\n",
      "Depth:4\n",
      "Train score: 0.203\n",
      "Test score: 0.201\n",
      "Depth:6\n",
      "Train score: 0.224\n",
      "Test score: 0.219\n",
      "Depth:8\n",
      "Train score: 0.242\n",
      "Test score: 0.228\n",
      "Depth:10\n",
      "Train score: 0.265\n",
      "Test score: 0.232\n",
      "Depth:12\n",
      "Train score: 0.295\n",
      "Test score: 0.232\n",
      "Depth:14\n",
      "Train score: 0.334\n",
      "Test score: 0.228\n",
      "Depth:16\n",
      "Train score: 0.378\n",
      "Test score: 0.222\n",
      "Depth:18\n",
      "Train score: 0.423\n",
      "Test score: 0.211\n"
     ]
    }
   ],
   "source": [
    "for max_depth in [2, 4, 6, 8, 10, 12, 14, 16, 18]:\n",
    "    model = RandomForestRegressor(max_depth= max_depth)\n",
    "    model.fit(x_train, y_train)\n",
    "    print(f\"Depth:{max_depth}\")\n",
    "    print(f\"Train score: {model.score(x_train, y_train):.3f}\")\n",
    "    print(f\"Test score: {model.score(x_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_IDADE: 0.043\n",
      "NUM_ANO_CONCLUIU: 0.000\n",
      "NUM_PESSOAS_RESIDENCIA: 0.056\n",
      "NUM_EMPREGADO_DOMESTICO: 0.099\n",
      "NUM_BANHEIRO: 0.143\n",
      "NUM_QUARTOS: 0.019\n",
      "NUM_CARRO: 0.031\n",
      "NUM_MOTO: 0.017\n",
      "NUM_GELADEIRA: 0.012\n",
      "NUM_FREEZER: 0.029\n",
      "NUM_MAQUINA_LAVAR: 0.011\n",
      "NUM_MAQUINA_SECAR: 0.014\n",
      "NUM_MICRO_ONDAS: 0.011\n",
      "NUM_LAVAR_LOUCA: 0.012\n",
      "NUM_TV: 0.023\n",
      "NUM_CELULAR: 0.028\n",
      "NUM_COMPUTADOR: 0.451\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(max_depth= 12)\n",
    "model.fit(x_train, y_train)\n",
    "for i, col in enumerate(X_columns_names):\n",
    "    print(f\"{col}: {model.feature_importances_[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
